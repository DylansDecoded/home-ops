---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: '3'

vars:
  BOOTSTRAP_RESOURCES_DIR: '{{.ROOT_DIR}}/.taskfiles/bootstrap/resources'

tasks:

  talos:
    desc: Bootstrap Talos [CLUSTER=main]
    dotenv: ['{{.CLUSTER_DIR}}/cluster.env']
    cmds:
      - for: { var: TALOS_NODES }
        cmd: >
          # sops exec-file --input-type yaml --output-type yaml {{.ITEM}} "minijinja-cli {}"
          # | talosctl --nodes {{base .ITEM | replace ".sops.yaml.j2" ""}} apply-config --insecure --file /dev/stdin
      - until talosctl --nodes {{.TALOS_CONTROLLER}} bootstrap; do sleep 5; done
      - talosctl kubeconfig --nodes {{.TALOS_CONTROLLER}} --force --force-context-name {{.CLUSTER}} {{.CLUSTER_DIR}}
    vars:
      TALOS_CONTROLLER:
        sh: talosctl config info --output json | jq --raw-output '.endpoints[]' | shuf -n 1
      TALOS_NODES:
        sh: ls {{.CLUSTER_DIR}}/talos/*.j2
    requires:
      vars: [CLUSTER]
    preconditions:
      - talosctl config info
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - which ls minijinja-cli sops talosctl

  apps:
    desc: Bootstrap Apps [CLUSTER=main]
    cmds:
      - until kubectl wait nodes --for=condition=Ready=False --all --timeout=10m; do sleep 5; done
      - helmfile --quiet --file {{.CLUSTER_DIR}}/bootstrap/apps/helmfile.yaml apply --skip-diff-on-install --suppress-diff
      - until kubectl wait nodes --for=condition=Ready --all --timeout=10m; do sleep 5; done
    requires:
      vars: [CLUSTER]
    preconditions:
      - talosctl config info
      - test -f {{.CLUSTER_DIR}}/talosconfig
      - test -f {{.CLUSTER_DIR}}/bootstrap/apps/helmfile.yaml
      - which helmfile kubectl

  # NOTE: Nodes must all be part of the Ceph cluster and Ceph disks must share the same disk model
  rook:
    desc: Bootstrap Rook-Ceph [CLUSTER=main] [MODEL=required]
    cmds:
      - minijinja-cli {{.BOOTSTRAP_RESOURCES_DIR}}/wipe-rook.yaml.j2 | kubectl apply --server-side --filename -
      - until kubectl --namespace default get job/wipe-rook &>/dev/null; do sleep 5; done
      - kubectl --namespace default wait job/wipe-rook --for=condition=complete --timeout=5m
      - stern --namespace default job/wipe-rook --no-follow
      - kubectl --namespace default delete job wipe-rook
    env:
      MODEL: '{{ .MODEL }}'
      NODE_COUNT:
        sh: talosctl config info --output json | jq --raw-output '.nodes | length'
    requires:
      vars: [CLUSTER, MODEL]
    preconditions:
      - test -f {{.BOOTSTRAP_RESOURCES_DIR}}/wipe-rook.yaml.j2
      - which kubectl minijinja-cli stern talosctl
  # rook:
  #   desc: Bootstrap Rook-Ceph [CLUSTER=main]
  #   cmds:
  #     - for: { var: nodes }
  #       task: rook-data
  #       vars:
  #         node: '{{.ITEM}}'

  #     - for: { var: k8s-0 }
  #       task: rook-disk
  #       vars:
  #         node: k8s-0
  #         disk: '{{.ITEM}}'

  #     - for: { var: k8s-1 }
  #       task: rook-disk
  #       vars:
  #         node: k8s-1
  #         disk: '{{.ITEM}}'

  #     - for: { var: k8s-2 }
  #       task: rook-disk
  #       vars:
  #         node: k8s-2
  #         disk: '{{.ITEM}}'

  #     - for: { var: k8s-3 }
  #       task: rook-disk
  #       vars:
  #        node: k8s-3
  #        disk: '{{.ITEM}}'

  #     - for: { var: k8s-4 }
  #       task: rook-disk
  #       vars:
  #        node: k8s-4
  #        disk: '{{.ITEM}}'

  #     - for: { var: k8s-5 }
  #       task: rook-disk
  #       vars:
  #         node: k8s-5
  #         disk: '{{.ITEM}}'

  #   vars:
  #     nodes: k8s-0 k8s-1 k8s-2 k8s-3 k8s-4 k8s-5
  #     k8s-0: ata-SAMSUNG_MZ7KM960HAHP-00005_S2HTNXAH300874
  #     k8s-1: ata-SAMSUNG_MZ7KM960HAHP-000FU_S2TLNX0J900056
  #     k8s-2: ata-SAMSUNG_MZ7KM960HAHP-00005_S2HTNX0H415045
  #     k8s-3: ata-SAMSUNG_MZ7KM960HAHP-00005_S2HTNX0H413743
  #     k8s-4: ata-SAMSUNG_MZ7KM960HAHP-00005_S2HTNX0H414441
  #     k8s-5: ata-SAMSUNG_MZ7KM960HMJP-00005_S3HSNWAJ300236B

  # rook-data:
  #   internal: true
  #   cmds:
  #     - envsubst < <(cat {{.BOOTSTRAP_RESOURCES_DIR}}/rook-data-job.tmpl.yaml) | kubectl apply -f -
  #     - bash {{.BOOTSTRAP_RESOURCES_DIR}}/wait-for-job.sh {{.job}} default
  #     - kubectl --namespace default wait job/{{.job}} --for condition=complete --timeout=1m
  #     - kubectl --namespace default logs job/{{.job}}
  #     - kubectl --namespace default delete job {{.job}}
  #   env:
  #     job: '{{.job}}'
  #     node: '{{.node}}'
  #   vars:
  #     job: wipe-data-{{.node}}
  #   preconditions:
  #     - test -f {{.BOOTSTRAP_RESOURCES_DIR}}/wait-for-job.sh
  #     - test -f {{.BOOTSTRAP_RESOURCES_DIR}}/rook-data-job.tmpl.yaml

  # rook-disk:
  #   internal: true
  #   cmds:
  #     - envsubst < <(cat {{.BOOTSTRAP_RESOURCES_DIR}}/rook-disk-job.tmpl.yaml) | kubectl apply -f -
  #     - bash {{.BOOTSTRAP_RESOURCES_DIR}}/wait-for-job.sh {{.job}} default
  #     - kubectl --namespace default wait job/{{.job}} --for condition=complete --timeout=1m
  #     - kubectl --namespace default logs job/{{.job}}
  #     - kubectl --namespace default delete job {{.job}}
  #   env:
  #     disk: /dev/disk/by-id/{{.disk}}
  #     job: '{{.job}}'
  #     node: '{{.node}}'
  #   vars:
  #     job: wipe-disk-{{.node}}
  #   preconditions:
  #     - test -f {{.BOOTSTRAP_RESOURCES_DIR}}/wait-for-job.sh
  #     - test -f {{.BOOTSTRAP_RESOURCES_DIR}}/rook-disk-job.tmpl.yaml

  flux:
    desc: Bootstrap Flux [CLUSTER=main]
    cmds:
      - kubectl apply --server-side --kustomize {{.CLUSTER_DIR}}/bootstrap/apps
      - for: { var: TEMPLATES }
        cmd: op run --env-file {{.CLUSTER_DIR}}/bootstrap/bootstrap.env --no-masking -- minijinja-cli {{.ITEM}} | kubectl apply --server-side --filename -
      - kubectl apply --server-side --kustomize {{.CLUSTER_DIR}}/flux/config
    vars:
      TEMPLATES:
        sh: ls {{.CLUSTER_DIR}}/bootstrap/apps/*.j2
    env:
      VAULT: '{{if eq .CLUSTER "main"}}kubernetes{{else}}{{.CLUSTER}}{{end}}' # ¯\_(ツ)_/¯
      FLUX_GITHUB_PUBLIC_KEYS:
        sh: curl -fsSL https://api.github.com/meta | jq --raw-output '"github.com "+.ssh_keys[]'
    requires:
      vars: [CLUSTER]
    preconditions:
      - op user get --me
      - which curl flux kubectl ls op
